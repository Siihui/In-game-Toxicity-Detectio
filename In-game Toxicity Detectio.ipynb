{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T1KYrIOomvx"
      },
      "source": [
        "## 1 Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZtXvjnoIwK",
        "outputId": "3e8f5552-24b3-45ec-f0ea-71a9b0a4d1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.stem.porter import *\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import FastText\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucDOdAzko9s4"
      },
      "source": [
        "### 1.0. Data collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlC7Tcq_pArx",
        "outputId": "4f190beb-0e06-44d0-94e7-bbe2e57188a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 26078\n",
            "Size of validation dataset: 500\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: Gg / SENTENCE: S\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = 'file_link'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.csv')  \n",
        "\n",
        "id = 'file_link'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('val_data.csv')  \n",
        "\n",
        "id = 'file_link'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.csv')  \n",
        "\n",
        "import pandas as pd\n",
        "training_data = pd.read_csv(\"file_dir")\n",
        "val_data = pd.read_csv(\"file_dir")\n",
        "testing_data = pd.read_csv(\"file_dir")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
        "print(\"Size of validation dataset: {0}\".format(len(testing_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data.iloc[-1,0], training_data.iloc[-1,1]))\n",
        "print(\"------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JUhcS30HrcAb",
        "outputId": "743157e8-f1bf-43b2-a8f1-0369c5a4235c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3aa3ee27-8c7c-4ab4-80f6-ef6d932512eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WTF</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wpe wpe</td>\n",
              "      <td>O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hahaha</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wtf</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aa3ee27-8c7c-4ab4-80f6-ef6d932512eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3aa3ee27-8c7c-4ab4-80f6-ef6d932512eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3aa3ee27-8c7c-4ab4-80f6-ef6d932512eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     sents labels\n",
              "0      wow      O\n",
              "1      WTF      T\n",
              "2  wpe wpe    O O\n",
              "3   hahaha      O\n",
              "4      wtf      T"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preview of the data in the csv file, which has two columns: \n",
        "# (1)sents - content (2)labels \n",
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zOTPOzwmirXz",
        "outputId": "b773d09e-41ae-4ad1-f895-9536cd105f65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-68901ac3-3dd0-4e9e-8a44-9b11336900c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WTF</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wpe wpe</td>\n",
              "      <td>O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hahaha</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wtf</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>EZ</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>aw</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>and telling im too much of a solo player</td>\n",
              "      <td>O O O O O O O S O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>He s never picked in pubs and the only bracket...</td>\n",
              "      <td>P O O O O O O O O O P O O O O O O O O S O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>2v5 game</td>\n",
              "      <td>O O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68901ac3-3dd0-4e9e-8a44-9b11336900c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68901ac3-3dd0-4e9e-8a44-9b11336900c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68901ac3-3dd0-4e9e-8a44-9b11336900c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                sents  \\\n",
              "0                                                 wow   \n",
              "1                                                 WTF   \n",
              "2                                             wpe wpe   \n",
              "3                                              hahaha   \n",
              "4                                                 wtf   \n",
              "..                                                ...   \n",
              "95                                                 EZ   \n",
              "96                                                 aw   \n",
              "97           and telling im too much of a solo player   \n",
              "98  He s never picked in pubs and the only bracket...   \n",
              "99                                           2v5 game   \n",
              "\n",
              "                                               labels  \n",
              "0                                                   O  \n",
              "1                                                   T  \n",
              "2                                                 O O  \n",
              "3                                                   O  \n",
              "4                                                   T  \n",
              "..                                                ...  \n",
              "95                                                  S  \n",
              "96                                                  O  \n",
              "97                                  O O O O O O O S O  \n",
              "98  P O O O O O O O O O P O O O O O O O O S O O O ...  \n",
              "99                                                O O  \n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZUQ17czrrRv"
      },
      "outputs": [],
      "source": [
        "# Extract the labels and posts and store into List\n",
        "# Get the list of training data (posts)\n",
        "training_sents=training_data['sents'].tolist()\n",
        "# Get the list of corresponding labels for the training data (posts)\n",
        "training_labels=training_data['labels'].tolist()\n",
        "\n",
        "# Get the list of testing data (posts)\n",
        "val_sents=val_data['sents'].tolist()\n",
        "# Get the list of corresponding labels for the testing data (posts)\n",
        "val_labels=val_data['labels'].tolist()\n",
        "\n",
        "# Get the list of testing data (posts)\n",
        "testing_sents=testing_data['sents'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJlUUzKqr7ZP"
      },
      "outputs": [],
      "source": [
        "# store labels in list\n",
        "training_labels_ls=[]\n",
        "for labels in training_labels:\n",
        "    tokenized_sentence = word_tokenize(labels)\n",
        "    training_labels_ls.append(tokenized_sentence)\n",
        "\n",
        "val_labels_ls=[]\n",
        "for labels in val_labels:\n",
        "    tokenized_sentence = word_tokenize(labels)\n",
        "    val_labels_ls.append(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_6TyOhW1_F"
      },
      "source": [
        "### 1.1 Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hSYQc1lP2Xt",
        "outputId": "88b470b5-9738-4b21-edee-f52ae6d672ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# preprocessing sentences\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.stem.porter import *\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# preprocessing training set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "train_tokensized_docs = []\n",
        "for sentense in training_sents:\n",
        "  tokenized_sentence = sentense.split(' ')\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  lemmatizes = [lemmatizer.lemmatize(word) for word in lower_tokens]\n",
        "  train_tokensized_docs.append(lemmatizes)\n",
        "\n",
        "# preprocessing validation set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "val_tokensized_docs = []\n",
        "for sentense in val_sents:\n",
        "  tokenized_sentence = sentense.split(' ')\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  lemmatizes = [lemmatizer.lemmatize(word) for word in lower_tokens]\n",
        "  val_tokensized_docs.append(lemmatizes)\n",
        "\n",
        "# preprocessing testing set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "test_tokensized_docs = []\n",
        "for sentense in testing_sents:\n",
        "  tokenized_sentence = sentense.split(' ')\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence] \n",
        "  lemmatizes = [lemmatizer.lemmatize(word) for word in lower_tokens]\n",
        "  test_tokensized_docs.append(lemmatizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOlHijZPtUOi"
      },
      "source": [
        "## 2 Input representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOw6s1a-ukTQ"
      },
      "source": [
        "### 2.1 Semantic Textual Feature Embedding: Word Embeddings (Word2Vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2LRHZkPOIsw"
      },
      "outputs": [],
      "source": [
        "# Generate word_to_ix and tag_to_ix\n",
        "word_to_ix = {}\n",
        "for sentence in train_tokensized_docs+val_tokensized_docs+test_tokensized_docs:\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix) \n",
        "word_list = list(word_to_ix.keys())   \n",
        "\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in training_labels_ls+val_labels_ls:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6E7CNUrtMyx",
        "outputId": "877f7fa5-ed2f-495d-83d9-fe5330a04a04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(10723, 50)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "EMBEDDING_DIM = 50\n",
        "gn_sg_model=Word2Vec(sentences=train_tokensized_docs,size=50,window=5,sg=1, workers=2,min_count=0)\n",
        "semantic_emb_dim = gn_sg_model.vector_size #50\n",
        "\n",
        "semantic_emb_table = []\n",
        "for i, word in enumerate(word_list):\n",
        "    if word in gn_sg_model:\n",
        "        semantic_emb_table.append(gn_sg_model[word])\n",
        "    else:\n",
        "        semantic_emb_table.append([0]*semantic_emb_dim)\n",
        "\n",
        "semantic_emb_table = np.array(semantic_emb_table) \n",
        "semantic_emb_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRUnCKFXffhq"
      },
      "source": [
        "### 2.2 Syntactic Textual Feature Embedding: PoS tag information, Dependency Path,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlxXmEuDGD0z"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# create pos and dependency \n",
        "def pos_dep(sents):\n",
        "  pos=[]\n",
        "  dep=[]\n",
        "  for sent in sents:\n",
        "    doc = nlp(sent)\n",
        "    pos_temp = []\n",
        "    dep_temp = []\n",
        "    for token in doc:\n",
        "      pos_temp.append(token.pos_)\n",
        "      dep_temp.append(token.dep_)\n",
        "    pos.append(pos_temp)\n",
        "    dep.append(dep_temp)\n",
        "  return pos,dep\n",
        "\n",
        "pos_to_ix = {}\n",
        "dep_to_ix = {}\n",
        "train_pos, train_dep = pos_dep(training_sents)\n",
        "val_pos, val_dep = pos_dep(val_sents)\n",
        "test_pos, test_dep = pos_dep(testing_sents)\n",
        "\n",
        "for poss in train_pos + val_pos + test_pos:\n",
        "  for pos in poss:\n",
        "    if pos not in pos_to_ix:\n",
        "      pos_to_ix[pos] = len(pos_to_ix)\n",
        "\n",
        "for deps in train_dep + val_dep + test_dep:\n",
        "  for dep in deps:\n",
        "    if dep not in dep_to_ix:\n",
        "      dep_to_ix[dep] = len(dep_to_ix)\n",
        "\n",
        "# convert into embedding vector\n",
        "pos_emb_table = np.eye(len(list(pos_to_ix.values())))\n",
        "dep_emb_table = np.eye(len(list(dep_to_ix.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkyDUQRKHpQ7",
        "outputId": "d0867952-eeb8-4e24-83d7-850fd02bbbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17, 17)\n",
            "(45, 45)\n"
          ]
        }
      ],
      "source": [
        "print(pos_emb_table.shape)\n",
        "print(dep_emb_table.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqUT6ZPFiE61"
      },
      "source": [
        "### 2.3 domain feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxLA3QFyeW-c",
        "outputId": "fed566dc-ed4b-4e1c-c480-e719cc2dea48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=ff96f040a282263f5f61832560ab5b7500f2dc187b20174f092d79d83e4a2fda\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "## Install and import the wikipedia library\n",
        "!pip install wikipedia\n",
        "import wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFkghgaJebDf"
      },
      "outputs": [],
      "source": [
        "# collect information from 'Dota' page\n",
        "documents_1 = [wikipedia.page('Dota', auto_suggest=False).content]\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wiki_tokensized_docs = []\n",
        "for sentense in documents_1:\n",
        "  tokenized_sentence = word_tokenize(sentense)\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  wiki_tokensized_docs.append(lower_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xhHHvKitupW"
      },
      "outputs": [],
      "source": [
        "# collect information from 'Dota 2' page\n",
        "documents_2 = [wikipedia.page('Dota 2', auto_suggest=False).content]\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sentense in documents_2:\n",
        "  tokenized_sentence = word_tokenize(sentense)\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  wiki_tokensized_docs.append(lower_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLovQJrAjRLa"
      },
      "outputs": [],
      "source": [
        "# collect information from 'Multiplayer online battle arena' page\n",
        "documents_3 = [wikipedia.page('Multiplayer online battle arena', auto_suggest=False).content]\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sentense in documents_3:\n",
        "  tokenized_sentence = word_tokenize(sentense)\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  wiki_tokensized_docs.append(lower_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhmlAAyfkZcu"
      },
      "outputs": [],
      "source": [
        "# collect information from 'Defense of the Ancients' page\n",
        "documents_4 = [wikipedia.page('Defense of the Ancients', auto_suggest=False).content]\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sentense in documents_4:\n",
        "  tokenized_sentence = word_tokenize(sentense)\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  wiki_tokensized_docs.append(lower_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaGagS1eURQa"
      },
      "outputs": [],
      "source": [
        "# collect information from 'Internet slang' page\n",
        "documents_5 = [wikipedia.page('Internet slang', auto_suggest=False).content]\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sentense in documents_5:\n",
        "  tokenized_sentence = word_tokenize(sentense)\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  wiki_tokensized_docs.append(lower_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brxp8Cf2U6ls"
      },
      "outputs": [],
      "source": [
        "# collect information from 'Glossary of video game terms' page\n",
        "documents_6 = [wikipedia.page('Glossary of video game terms', auto_suggest=False).content]\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sentense in documents_6:\n",
        "  tokenized_sentence = word_tokenize(sentense)\n",
        "  lower_tokens = [t.lower() for t in tokenized_sentence]\n",
        "  wiki_tokensized_docs.append(lower_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paObUy64x7mg"
      },
      "outputs": [],
      "source": [
        "# create look up table for wiki_doc\n",
        "domain_word_set = set() \n",
        "for sent in wiki_tokensized_docs:\n",
        "    for word in sent:\n",
        "        domain_word_set.add(word)\n",
        "        \n",
        "domain_word_set.add('[PAD]')\n",
        "domain_word_set.add('[UNKNOWN]')\n",
        "domain_word_list = list(domain_word_set) \n",
        "domain_word_list.sort()\n",
        "domain_word_index = {}\n",
        "ind = 0\n",
        "for word in domain_word_list:\n",
        "    domain_word_index[word] = ind\n",
        "    ind += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq_V--fuXm30",
        "outputId": "f9107daf-a2ef-4cc5-e338-90b7686929b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5907"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The corpus only contains few words\n",
        "len(domain_word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KMRVZjJYPII",
        "outputId": "4959811d-1869-4e7c-deee-531bc051511c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(10723, 25)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create domain feature embedding\n",
        "from gensim.models import Word2Vec\n",
        "domain_feature_wv_sg_model=Word2Vec(sentences=wiki_tokensized_docs,size=25,window=5,sg=1, workers=2,min_count=0)\n",
        "domain_feature_wv_sg_model_emb_dim = domain_feature_wv_sg_model.vector_size #25\n",
        "domain_feature_wv_sg_model_emb_table = []\n",
        "for i, word in enumerate(word_list):\n",
        "    if word in domain_feature_wv_sg_model:\n",
        "        domain_feature_wv_sg_model_emb_table.append(domain_feature_wv_sg_model[word])\n",
        "    else:\n",
        "        domain_feature_wv_sg_model_emb_table.append([0]*domain_feature_wv_sg_model_emb_dim)\n",
        "\n",
        "domain_feature_wv_sg_model_emb_table = np.array(domain_feature_wv_sg_model_emb_table) \n",
        "domain_feature_wv_sg_model_emb_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ige3zUjqpUgo"
      },
      "source": [
        "### 2.4 convert dataset into indexs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX1nrc4vI9o9"
      },
      "outputs": [],
      "source": [
        "def to_index(data, to_ix):\n",
        "    input_index_list = [] \n",
        "    for sent in data:\n",
        "        input_sent=[]\n",
        "        for w in sent:\n",
        "            if w not in to_ix:\n",
        "                w ='[UNKNOWN]'\n",
        "            input_sent.append(to_ix[w])           \n",
        "        input_index_list.append(input_sent)\n",
        "    return input_index_list\n",
        "\n",
        "train_input_index =  to_index(train_tokensized_docs,word_to_ix)  #[[0], [1, 2, 3, 4, 5, 6, 7, 8, 9]..]\n",
        "train_output_index = to_index(training_labels_ls,tag_to_ix)\n",
        "train_dep_index = to_index(train_dep,dep_to_ix)\n",
        "train_pos_index =  to_index(train_pos,pos_to_ix)\n",
        "train_domain_index =  to_index(train_tokensized_docs,domain_word_index)\n",
        "\n",
        "val_input_index = to_index(val_tokensized_docs,word_to_ix)\n",
        "val_output_index = to_index(val_labels_ls,tag_to_ix)\n",
        "val_dep_index = to_index(val_dep,dep_to_ix)\n",
        "val_pos_index =  to_index(val_pos,pos_to_ix)\n",
        "val_domain_index =  to_index(train_tokensized_docs,domain_word_index)\n",
        "\n",
        "test_input_index = to_index(test_tokensized_docs,word_to_ix)\n",
        "test_dep_index = to_index(test_dep,dep_to_ix)\n",
        "test_pos_index =  to_index(test_pos,pos_to_ix)\n",
        "test_domain_index =  to_index(train_tokensized_docs,domain_word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h-qLZA6rlHY"
      },
      "source": [
        "## 3. Slot tagging model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYUTyNoHAIil"
      },
      "source": [
        "### 3.1 Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SONFiUuGPtm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def cal_f1(pred,ground_truth):\n",
        "  pred = np.array(pred)\n",
        "  ground_truth = np.array(ground_truth)\n",
        "  F1_class = f1_score(ground_truth,pred,average=None)\n",
        "  F1 = f1_score(ground_truth,pred,average='micro')\n",
        "  F1_O = f1_score(ground_truth[ground_truth==tag_to_ix['O']], pred[ground_truth==tag_to_ix['O']],average='micro')\n",
        "  F1_T = f1_score(ground_truth[ground_truth==tag_to_ix['T']], pred[ground_truth==tag_to_ix['T']],average='micro')\n",
        "  F1_P = f1_score(ground_truth[ground_truth==tag_to_ix['P']], pred[ground_truth==tag_to_ix['P']],average='micro')\n",
        "  F1_SEPA = f1_score(ground_truth[ground_truth==tag_to_ix['SEPA']], pred[ground_truth==tag_to_ix['SEPA']],average='micro')\n",
        "  F1_S = f1_score(ground_truth[ground_truth==tag_to_ix['S']], pred[ground_truth==tag_to_ix['S']],average='micro')\n",
        "  F1_C = f1_score(ground_truth[ground_truth==tag_to_ix['C']], pred[ground_truth==tag_to_ix['C']],average='micro')\n",
        "  F1_D = f1_score(ground_truth[ground_truth==tag_to_ix['D']], pred[ground_truth==tag_to_ix['D']],average='micro')\n",
        "  \n",
        "  \n",
        "  return F1,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4wXirtoFVUr"
      },
      "outputs": [],
      "source": [
        "# Author: Robert Guthrie\n",
        "# Code from week 9 Lab  \n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfvPB5kvRczq"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "HIDDEN_DIM = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc2qSITUFisc"
      },
      "outputs": [],
      "source": [
        "def train_baseline(embedding_dim,hidden_dim):\n",
        "    model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, embedding_dim, hidden_dim).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "    \n",
        "\n",
        "    for epoch in range(2):\n",
        "        model.train()\n",
        "        train_loss =0\n",
        "        predict =[]\n",
        "        gt = []\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "        \n",
        "            model.zero_grad()\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)  \n",
        "         \n",
        "            \n",
        "            _,output = model(sentence_in)\n",
        "            predict.extend(output)\n",
        "            gt.extend(targets)\n",
        "\n",
        "            loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        F1_train,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D = cal_f1(predict,gt)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        predict =[]\n",
        "        gt = []\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)  \n",
        " \n",
        "            _,output = model(sentence_in)\n",
        "            predict.extend(output)\n",
        "            gt.extend(targets)\n",
        "            loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D = cal_f1(predict,gt)\n",
        "        print(f'Epoch: {epoch+1}, Training loss: {train_loss:.4f}, Train f1: {F1_train:.3f}, Val loss: {val_loss:.4f}, Val f1: {F1_val:.3f}')\n",
        "    return F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xx0LfA4AQMU"
      },
      "source": [
        "### 3.2 Bilstm-attention-crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJY3eFD0mUJN"
      },
      "outputs": [],
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4UpLNAD93tq"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_CRF_ATTN(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim,attn='Scaled Dot Product' ,CRF =True,LSTM_layer=1, pos_dim=0, dep_dim = 0, domain_dim = 0 ):\n",
        "        super(BiLSTM_CRF_ATTN, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.pos_dim = pos_dim\n",
        "        self.dep_dim = dep_dim        \n",
        "        self.domain_dim = domain_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = len(word_to_ix)\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.LSTM_layer = LSTM_layer\n",
        "        self.CRF= CRF\n",
        "        self.attn = attn\n",
        "\n",
        "        self.word_embeds = nn.Embedding(self.vocab_size, embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(semantic_emb_table))\n",
        "\n",
        "        if pos_dim >0:\n",
        "          self.pos_embeds = nn.Embedding(len(pos_to_ix), pos_dim)\n",
        "          self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_emb_table))\n",
        "\n",
        "        if dep_dim >0:\n",
        "          self.dep_embeds = nn.Embedding(len(dep_to_ix), dep_dim)\n",
        "          self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_emb_table))\n",
        "\n",
        "        if domain_dim >0:\n",
        "          self.domain_embeds = nn.Embedding(self.vocab_size, domain_dim)\n",
        "          self.domain_embeds.weight.data.copy_(torch.from_numpy(domain_feature_wv_sg_model_emb_table))\n",
        "        \n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim+pos_dim+dep_dim+domain_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "        \n",
        "        self.lstm_layers = []\n",
        "        self.lstm_layers.append(self.lstm)\n",
        "\n",
        "        if self.LSTM_layer >1:\n",
        "            for _ in range(1,self.LSTM_layer):\n",
        "                  self.lstm_layers.append(nn.LSTM(hidden_dim, hidden_dim // 2,num_layers=1, bidirectional=True)) \n",
        "        self.lstm_layers = nn.ModuleList(self.lstm_layers)\n",
        "\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        self.transitions.data[self.tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, self.tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        if self.attn:\n",
        "            self.q = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.k = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.v = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "            if self.attn =='General':\n",
        "                self.general = nn.Linear(hidden_dim,hidden_dim)\n",
        "        else:\n",
        "            self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "          \n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "        \n",
        "    def attention(self, lstm_out):\n",
        "        lstm_out = lstm_out.squeeze(1)\n",
        "        query = self.q(lstm_out).unsqueeze(0)\n",
        "        key = self.k(lstm_out).transpose(1,0).unsqueeze(0)\n",
        "        if self.attn =='General':\n",
        "            key = self.general(key)\n",
        "        value = self.v(lstm_out).unsqueeze(0)\n",
        "        if self.attn == 'General':\n",
        "            attn_w = F.softmax(torch.bmm(query,key),dim = -1) \n",
        "        if self.attn == 'Scale Dot Product':\n",
        "            attn_w = F.softmax(torch.bmm(query,key)/torch.sqrt(torch.Tensor([self.hidden_dim])),dim = -1)\n",
        "        if self.attn == 'Dot Product':\n",
        "            attn_w = F.softmax(torch.bmm(query,key),dim = -1)    \n",
        "          \n",
        "        attn_output = torch.bmm(attn_w, value).transpose(1,0)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "    def _get_lstm_features(self, sentence, pos, sep, domain):\n",
        "        \n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        if self.pos_dim >0:\n",
        "            pos_embeds = self.pos_embeds(pos).view(len(sentence), 1, -1)\n",
        "            embeds = torch.cat((embeds,pos_embeds),-1)\n",
        "        if self.dep_dim >0:\n",
        "            dep_embeds = self.dep_embeds(dep).view(len(sentence), 1, -1)\n",
        "            embeds = torch.cat((embeds,dep_embeds),-1)\n",
        "        if self.domain_dim >0:\n",
        "            domain_embeds = self.domain_embeds(domain).view(len(sentence), 1, -1)\n",
        "            embeds = torch.cat((embeds,domain_embeds),-1)\n",
        "        lstm = self.lstm_layers[0]\n",
        "        lstm_out, self.hidden = lstm(embeds, self.hidden) #first LSTM layer\n",
        "        if self.LSTM_layer >1:\n",
        "      \n",
        "            for lstm in self.lstm_layers[1:]:\n",
        "                self.hidden = self.init_hidden()\n",
        "                lstm_out, self.hidden = lstm(lstm_out, self.hidden)\n",
        "        if self.attn:\n",
        "            attn_output = self.attention(lstm_out)\n",
        "            lstm_out = torch.cat((attn_output,lstm_out),-1)\n",
        "            lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
        "        else:\n",
        "            lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags,pos, dep, domain):\n",
        "        feats = self._get_lstm_features(sentence,pos, dep, domain)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, pos,  dep, domain):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence, pos, dep,domain)\n",
        "        if self.CRF:\n",
        "            score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        else:\n",
        "            tag_seq = torch.argmax(F.softmax(lstm_feats, -1), -1)\n",
        "      \n",
        "        return tag_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrWTHSVNKuYO"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "HIDDEN_DIM = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JajxfM_FKozu"
      },
      "outputs": [],
      "source": [
        "def train(embedding_dim,hidden_dim,attn,CRF,LSTM_layer,pos_dim, dep_dim , domain_dim  ):\n",
        "    model = BiLSTM_CRF_ATTN(embedding_dim, hidden_dim,attn='Scale Dot Product',CRF =True,LSTM_layer =1, pos_dim=0, dep_dim = 0, domain_dim = 0)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "    for epoch in range(2):\n",
        "        model.train()\n",
        "        train_loss =0\n",
        "        predict =[]\n",
        "        gt = []\n",
        "        for i, idxs in enumerate(train_input_index):\n",
        "            tags_index = train_output_index[i]\n",
        "          \n",
        "            dep_index = train_dep_index[i]\n",
        "            pos_index = train_pos_index[i]\n",
        "            domain_index = train_domain_index[i]\n",
        "            model.zero_grad()\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)  \n",
        "            dep_in = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "            pos_in = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "            domain_in = torch.tensor(domain_index, dtype=torch.long).to(device)\n",
        "            \n",
        "            output = model(sentence_in, pos_in, dep_in,domain_in)\n",
        "            predict.extend(output)\n",
        "            gt.extend(targets)\n",
        "\n",
        "            loss = model.neg_log_likelihood(sentence_in, targets,pos_in,dep_in,domain_in)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        F1_train,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D = cal_f1(predict,gt)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        predict =[]\n",
        "        gt = []\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "            tags_index = val_output_index[i]\n",
        "        \n",
        "            dep_index = val_dep_index[i]\n",
        "            pos_index = val_pos_index[i]\n",
        "            domain_index = val_domain_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)  \n",
        "            dep_in = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "            pos_in = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "            domain_in = torch.tensor(domain_index, dtype=torch.long).to(device)\n",
        "        \n",
        "            output = model(sentence_in, pos_in, dep_in,domain_in)\n",
        "            predict.extend(output)\n",
        "            gt.extend(targets)\n",
        "            loss = model.neg_log_likelihood(sentence_in, targets,pos_in,dep_in,domain_in)\n",
        "            val_loss+=loss.item()\n",
        "        F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D = cal_f1(predict,gt)\n",
        "        print(f'Epoch: {epoch+1}, Training loss: {train_loss:.4f}, Train f1: {F1_train:.3f}, Val loss: {val_loss:.4f}, Val f1: {F1_val:.3f}')\n",
        "    return F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9WaC8hDr3wK"
      },
      "source": [
        "## 4. Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t83ZDB_r54K"
      },
      "source": [
        "### 4.1 Baseline model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-xrYj1zJf_W",
        "outputId": "b88b4c1c-e9b4-4b83-d399-5606c38ce642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training loss: 35203.2436, Train f1: 0.902, Val loss: 5165.5867, Val f1: 0.962\n",
            "Epoch: 2, Training loss: 11041.0057, Train f1: 0.973, Val loss: 3273.2174, Val f1: 0.978\n",
            "Baseline model performance\n",
            "------------------------------------------------------------------------------\n",
            "Model \t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|  F1(D)|\n",
            "------------------------------------------------------------------------------\n",
            "Model1\t| 0.978  | 0.987  | 0.922 |  0.982  | 1.000     | 0.971  |0.921   |0.761 |\n"
          ]
        }
      ],
      "source": [
        "F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D, baseline = train_baseline(embedding_dim = 50,\n",
        "                                                                        hidden_dim =100)\n",
        "print('Baseline model performance')\n",
        "print('-'*78)\n",
        "print('Model \\t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|  F1(D)|')\n",
        "print('-'*78)\n",
        "print(f'Model1\\t| {F1_val:.3f}  | {F1_O:.3f}  | {F1_T:.3f} |  {F1_P:.3f}  | {F1_SEPA:.3f}     | {F1_S:.3f}  |{F1_C:.3f}   |{F1_D:.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TANKrVcOr-1v"
      },
      "source": [
        "### 4.2 Ablation Study - with/without CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MeYhj8Ge06S",
        "outputId": "3bf5be60-f96c-4354-fafb-cbfde4887ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with CRF\n",
            "Epoch: 1, Training loss: 28958.7756, Train f1: 0.919, Val loss: 2594.6808, Val f1: 0.984\n",
            "Epoch: 2, Training loss: 4947.6039, Train f1: 0.989, Val loss: 1797.8024, Val f1: 0.989\n",
            "without CRF\n",
            "Epoch: 1, Training loss: 29039.2075, Train f1: 0.920, Val loss: 2639.0187, Val f1: 0.982\n",
            "Epoch: 2, Training loss: 5141.3719, Train f1: 0.987, Val loss: 1732.6527, Val f1: 0.990\n",
            "Model with & without CRF\n",
            "------------------------------------------------------------------------------\n",
            "Model \t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|  F1(D)|\n",
            "------------------------------------------------------------------------------\n",
            "Model1\t| 0.989  | 0.995  | 0.963 |  0.990  | 1.000     | 0.985  |0.963   |0.874 |\n",
            "Model2\t| 0.990  | 0.995  | 0.965 |  0.990  | 1.000     | 0.986  |0.965   |0.859 |\n"
          ]
        }
      ],
      "source": [
        "#test\n",
        "print('with CRF')\n",
        "F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D, model = train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer = 1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0 \n",
        "                                                    )\n",
        "\n",
        "print('without CRF')\n",
        "F1_val2,F1_O2,F1_T2,F1_P2,F1_SEPA2,F1_S2,F1_C2,F1_D2, model= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Dot Product',\n",
        "                                                    CRF =False,\n",
        "                                                    LSTM_layer = 1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0 \n",
        "                                                    )\n",
        "\n",
        "\n",
        "print('Model with & without CRF')\n",
        "print('-'*78)\n",
        "print('Model \\t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|  F1(D)|')\n",
        "print('-'*78)\n",
        "print(f'Model1\\t| {F1_val:.3f}  | {F1_O:.3f}  | {F1_T:.3f} |  {F1_P:.3f}  | {F1_SEPA:.3f}     | {F1_S:.3f}  |{F1_C:.3f}   |{F1_D:.3f} |')\n",
        "print(f'Model2\\t| {F1_val2:.3f}  | {F1_O2:.3f}  | {F1_T2:.3f} |  {F1_P2:.3f}  | {F1_SEPA2:.3f}     | {F1_S2:.3f}  |{F1_C2:.3f}   |{F1_D2:.3f} |')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXlvaClusDav"
      },
      "source": [
        "### 4.3 Ablation Study - different Stacked layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umOkJD-BO2s1",
        "outputId": "87247fb7-3cc4-4502-8f78-36d7aa7f68d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 LSTM layer\n",
            "Epoch: 1, Training loss: 21548.0279, Train f1: 0.936, Val loss: 2796.0397, Val f1: 0.978\n",
            "Epoch: 2, Training loss: 6419.0457, Train f1: 0.983, Val loss: 1949.9075, Val f1: 0.985\n",
            "2 LSTM layer\n",
            "Epoch: 1, Training loss: 22271.9268, Train f1: 0.934, Val loss: 2787.1431, Val f1: 0.978\n",
            "Epoch: 2, Training loss: 6456.4996, Train f1: 0.982, Val loss: 1933.3702, Val f1: 0.985\n",
            "3 LSTM layer\n",
            "Epoch: 1, Training loss: 21793.4215, Train f1: 0.935, Val loss: 2751.6435, Val f1: 0.978\n",
            "Epoch: 2, Training loss: 6365.4606, Train f1: 0.983, Val loss: 1941.8931, Val f1: 0.985\n",
            "Model with different number of LSTM layers\n",
            "Model \t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|   F1(D)|\n",
            "------------------------------------------------------------------------------\n",
            "Model1\t| 0.985  | 0.990  | 0.956 |  0.991  | 1.000     | 0.976  |0.951   |0.834  |\n",
            "Model3\t| 0.985  | 0.991  | 0.954 |  0.992  | 1.000     | 0.977  |0.950   |0.799 |\n",
            "Model4\t| 0.985  | 0.991  | 0.956 |  0.990  | 1.000     | 0.978  |0.945   |0.847 |\n"
          ]
        }
      ],
      "source": [
        "# stack layer\n",
        "print('1 LSTM layer')\n",
        "F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D, model1 = train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0 \n",
        "                                                    )\n",
        "print('2 LSTM layer')\n",
        "F1_val3,F1_O3,F1_T3,F1_P3,F1_SEPA3,F1_S3,F1_C3,F1_D3, model3= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer=2,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0)\n",
        "print('3 LSTM layer')\n",
        "F1_val4,F1_O4,F1_T4,F1_P4,F1_SEPA4,F1_S4,F1_C4,F1_D4, model4= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =3,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0)\n",
        "print('Model with different number of LSTM layers')\n",
        "print('Model \\t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|   F1(D)|')\n",
        "print('-'*78)\n",
        "print(f'Model1\\t| {F1_val:.3f}  | {F1_O:.3f}  | {F1_T:.3f} |  {F1_P:.3f}  | {F1_SEPA:.3f}     | {F1_S:.3f}  |{F1_C:.3f}   |{F1_D:.3f}  |')\n",
        "print(f'Model3\\t| {F1_val3:.3f}  | {F1_O3:.3f}  | {F1_T3:.3f} |  {F1_P3:.3f}  | {F1_SEPA3:.3f}     | {F1_S3:.3f}  |{F1_C3:.3f}   |{F1_D3:.3f} |')\n",
        "print(f'Model4\\t| {F1_val4:.3f}  | {F1_O4:.3f}  | {F1_T4:.3f} |  {F1_P4:.3f}  | {F1_SEPA4:.3f}     | {F1_S4:.3f}  |{F1_C4:.3f}   |{F1_D4:.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBngtZifsGlP"
      },
      "source": [
        "### 4.3 Ablation Study - different attention strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSYAJpb0axWs",
        "outputId": "f11b8d35-c6ab-468e-a3f6-b2290ee1b901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without attention\n",
            "Epoch: 1, Training loss: 30853.1669, Train f1: 0.915, Val loss: 2659.1881, Val f1: 0.983\n",
            "Epoch: 2, Training loss: 5239.0682, Train f1: 0.988, Val loss: 1770.4932, Val f1: 0.989\n",
            "\n",
            "With attention (Scale Dot Product)\n",
            "Epoch: 1, Training loss: 30888.2661, Train f1: 0.914, Val loss: 2585.1871, Val f1: 0.984\n",
            "Epoch: 2, Training loss: 5172.0691, Train f1: 0.988, Val loss: 1806.3862, Val f1: 0.989\n",
            "\n",
            "With attention (Dot Product)\n",
            "Epoch: 1, Training loss: 30843.0407, Train f1: 0.915, Val loss: 2717.4818, Val f1: 0.983\n",
            "Epoch: 2, Training loss: 5438.1814, Train f1: 0.987, Val loss: 1925.6281, Val f1: 0.989\n",
            "\n",
            "With attention (General)\n",
            "Epoch: 1, Training loss: 29570.9023, Train f1: 0.919, Val loss: 2624.8202, Val f1: 0.984\n",
            "Epoch: 2, Training loss: 5106.6847, Train f1: 0.989, Val loss: 1855.6392, Val f1: 0.990\n",
            "\n",
            "Model with or without attention\n",
            "------------------------------------------------------------------------------\n",
            "Model \t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|   F1(D)|\n",
            "------------------------------------------------------------------------------\n",
            "Model1\t| 0.989  | 0.995  | 0.963  |  0.990  | 1.000     | 0.984  |0.965   |0.857 |\n",
            "Model5\t| 0.989  | 0.995  | 0.965  |  0.991  | 1.000     | 0.976  |0.963   |0.905 |\n",
            "Model6\t| 0.989  | 0.995  | 0.963  |  0.990  |   1.000   | 0.984  | 0.961  | 0.874 |\n",
            "Model7\t| 0.990  | 0.995  | 0.965  |  0.990  |   1.000   | 0.986  | 0.965  | 0.879 |\n"
          ]
        }
      ],
      "source": [
        "print('Without attention')\n",
        "F1_val,F1_O,F1_T,F1_P,F1_SEPA,F1_S,F1_C,F1_D, model = train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn =None,\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer = 1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0 )\n",
        "print('\\nWith attention (Scaled Dot Product)')\n",
        "F1_val5,F1_O5,F1_T5,F1_P5,F1_SEPA5,F1_S5,F1_C5,F1_D5, model5= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer = 1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0)\n",
        "print('\\nWith attention (Dot Product)')\n",
        "F1_val6,F1_O6,F1_T6,F1_P6,F1_SEPA6,F1_S6,F1_C6,F1_D6, model6= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer = 1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0)\n",
        "\n",
        "print('\\nWith attention (General)')\n",
        "F1_val7,F1_O7,F1_T7,F1_P7,F1_SEPA7,F1_S7,F1_C7,F1_D7, model7= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='General',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer = 1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0 \n",
        "                                                    )\n",
        "print('\\nModel with or without attention')\n",
        "print('-'*78)\n",
        "print('Model \\t|   F1   |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|   F1(D)|')\n",
        "print('-'*78)\n",
        "print(f'Model1\\t| {F1_val:.3f}  | {F1_O:.3f}  | {F1_T:.3f}  |  {F1_P:.3f}  | {F1_SEPA:.3f}     | {F1_S:.3f}  |{F1_C:.3f}   |{F1_D:.3f} |')\n",
        "print(f'Model5\\t| {F1_val5:.3f}  | {F1_O5:.3f}  | {F1_T5:.3f}  |  {F1_P5:.3f}  | {F1_SEPA5:.3f}     | {F1_S5:.3f}  |{F1_C5:.3f}   |{F1_D5:.3f} |')\n",
        "print(f'Model6\\t| {F1_val6:.3f}  | {F1_O6:.3f}  | {F1_T6:.3f}  |  {F1_P6:.3f}  |   {F1_SEPA6:.3f}   | {F1_S6:.3f}  | {F1_C6:.3f}  | {F1_D6:.3f} |')\n",
        "print(f'Model7\\t| {F1_val7:.3f}  | {F1_O7:.3f}  | {F1_T7:.3f}  |  {F1_P7:.3f}  |   {F1_SEPA7:.3f}   | {F1_S7:.3f}  | {F1_C7:.3f}  | {F1_D7:.3f} |')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pclmLU2zsnOy"
      },
      "source": [
        "### 4.4 Ablation Study - different input embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-sSIKYKU5MQ",
        "outputId": "29359abd-b7ed-4400-bb79-f67e72992fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Embedding\n",
            "Epoch: 1, Training loss: 29288.7894, Train f1: 0.919, Val loss: 2605.0506, Val f1: 0.985\n",
            "Epoch: 2, Training loss: 4954.5498, Train f1: 0.989, Val loss: 1854.6282, Val f1: 0.989\n",
            "\n",
            "Word Embedding + Pos tag\n",
            "Epoch: 1, Training loss: 28846.5859, Train f1: 0.920, Val loss: 2553.1524, Val f1: 0.985\n",
            "Epoch: 2, Training loss: 4857.5607, Train f1: 0.989, Val loss: 1758.8777, Val f1: 0.990\n",
            "\n",
            "Word Embedding + Dependency path\n",
            "Epoch: 1, Training loss: 29245.7120, Train f1: 0.918, Val loss: 2518.3720, Val f1: 0.985\n",
            "Epoch: 2, Training loss: 5180.1621, Train f1: 0.988, Val loss: 1805.4840, Val f1: 0.990\n",
            "\n",
            " Word Embedding + Domain\n",
            "Epoch: 1, Training loss: 30664.6627, Train f1: 0.914, Val loss: 2741.2327, Val f1: 0.981\n",
            "Epoch: 2, Training loss: 5436.2001, Train f1: 0.987, Val loss: 1892.6179, Val f1: 0.989\n",
            "\n",
            " Word Embedding + Pos tag + Domain\n",
            "Epoch: 1, Training loss: 29517.8671, Train f1: 0.918, Val loss: 2572.8791, Val f1: 0.983\n",
            "Epoch: 2, Training loss: 4934.4205, Train f1: 0.988, Val loss: 1726.0682, Val f1: 0.990\n",
            "\n",
            " Word Embedding + Dependency path + Domain\n",
            "Epoch: 1, Training loss: 29944.4897, Train f1: 0.915, Val loss: 2622.0172, Val f1: 0.982\n",
            "Epoch: 2, Training loss: 4991.0006, Train f1: 0.988, Val loss: 1705.7852, Val f1: 0.990\n",
            "\n",
            " Word Embedding + POS+ Dependency path + Domain\n",
            "Epoch: 1, Training loss: 30313.6239, Train f1: 0.915, Val loss: 2751.1742, Val f1: 0.982\n",
            "Epoch: 2, Training loss: 5531.0783, Train f1: 0.987, Val loss: 1853.8319, Val f1: 0.989\n",
            "\n",
            "Input Embedding Performance\n",
            "------------------------------------------------------------------------------\n",
            "Model \t|    F1  |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|   F1(D)|\n",
            "------------------------------------------------------------------------------\n",
            "Model5\t| 0.989  | 0.995  | 0.963  |  0.989  | 1.000     | 0.984  |0.966   |0.849 |\n",
            "Model8\t| 0.990  | 0.995  | 0.965  |  0.990  |   1.000   | 0.985  | 0.962  | 0.887 |\n",
            "Model9\t| 0.990  | 0.995  | 0.963  |  0.991  |   1.000   | 0.983  | 0.960  | 0.912 |\n",
            "Model10\t| 0.989  | 0.995  | 0.962  |  0.990  |   1.000   | 0.982  | 0.960  | 0.887 |\n",
            "Model11\t| 0.990  | 0.995  | 0.965  |  0.989  | 1.000     | 0.988  |0.963   |0.905 |\n",
            "Model12\t| 0.990  | 0.995  | 0.964  |  0.989  |   1.000   | 0.986  | 0.965  | 0.884 |\n",
            "Model13\t| 0.989  | 0.995  | 0.963  |  0.991  | 1.000     | 0.985  |0.959   |0.877 |\n"
          ]
        }
      ],
      "source": [
        "print('Word Embedding')\n",
        "F1_val5,F1_O5,F1_T5,F1_P5,F1_SEPA5,F1_S5,F1_C5,F1_D5, model5= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0\n",
        "                                                    )\n",
        "\n",
        "print('\\nWord Embedding + Pos tag')\n",
        "F1_val8,F1_O8,F1_T8,F1_P8,F1_SEPA8,F1_S8,F1_C8,F1_D8, model8= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= pos_emb_table.shape[1], \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=0\n",
        "                                                    )\n",
        "\n",
        "print('\\nWord Embedding + Dependency path')\n",
        "F1_val9,F1_O9,F1_T9,F1_P9,F1_SEPA9,F1_S9,F1_C9,F1_D9, model9= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =dep_emb_table.shape[1], \n",
        "                                                    domain_dim=0)\n",
        "print('\\n Word Embedding + Domain')\n",
        "F1_val10,F1_O10,F1_T10,F1_P10,F1_SEPA10,F1_S10,F1_C10,F1_D10, model10= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=domain_feature_wv_sg_model_emb_table.shape[1])\n",
        "print('\\n Word Embedding + Pos tag + Domain')\n",
        "F1_val11,F1_O11,F1_T11,F1_P11,F1_SEPA11,F1_S11,F1_C11,F1_D11, model11= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= pos_emb_table.shape[1], \n",
        "                                                    dep_dim =0, \n",
        "                                                    domain_dim=domain_feature_wv_sg_model_emb_table.shape[1] \n",
        "                                                    )\n",
        "print('\\n Word Embedding + Dependency path + Domain')\n",
        "F1_val12,F1_O12,F1_T12,F1_P12,F1_SEPA12,F1_S12,F1_C12,F1_D12, model12= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= 0, \n",
        "                                                    dep_dim =dep_emb_table.shape[1], \n",
        "                                                    domain_dim=domain_feature_wv_sg_model_emb_table.shape[1] \n",
        "                                                    )\n",
        "\n",
        "print('\\n Word Embedding + POS+ Dependency path + Domain')\n",
        "F1_val13,F1_O13,F1_T13,F1_P13,F1_SEPA13,F1_S13,F1_C13,F1_D13, model13= train(embedding_dim = semantic_emb_table.shape[1],\n",
        "                                                    hidden_dim = 100,\n",
        "                                                    attn ='Scaled Dot Product',\n",
        "                                                    CRF =True,\n",
        "                                                    LSTM_layer =1,\n",
        "                                                    pos_dim= pos_emb_table.shape[1], \n",
        "                                                    dep_dim =dep_emb_table.shape[1], \n",
        "                                                    domain_dim=domain_feature_wv_sg_model_emb_table.shape[1] \n",
        "                                                    )\n",
        "print('\\nInput Embedding Performance')\n",
        "print('-'*78)\n",
        "print('Model \\t|    F1  |   F1(O)|   F1(T)|   F1(P)|   F1(SEPA)|   F1(S)|   F1(C)|   F1(D)|')\n",
        "print('-'*78)\n",
        "print(f'Model5\\t| {F1_val5:.3f}  | {F1_O5:.3f}  | {F1_T5:.3f}  |  {F1_P5:.3f}  | {F1_SEPA5:.3f}     | {F1_S5:.3f}  |{F1_C5:.3f}   |{F1_D5:.3f} |')\n",
        "print(f'Model8\\t| {F1_val8:.3f}  | {F1_O8:.3f}  | {F1_T8:.3f}  |  {F1_P8:.3f}  |   {F1_SEPA8:.3f}   | {F1_S8:.3f}  | {F1_C8:.3f}  | {F1_D8:.3f} |')\n",
        "print(f'Model9\\t| {F1_val9:.3f}  | {F1_O9:.3f}  | {F1_T9:.3f}  |  {F1_P9:.3f}  |   {F1_SEPA9:.3f}   | {F1_S9:.3f}  | {F1_C9:.3f}  | {F1_D9:.3f} |')\n",
        "print(f'Model10\\t| {F1_val10:.3f}  | {F1_O10:.3f}  | {F1_T10:.3f}  |  {F1_P10:.3f}  |   {F1_SEPA10:.3f}   | {F1_S10:.3f}  | {F1_C10:.3f}  | {F1_D10:.3f} |')\n",
        "print(f'Model11\\t| {F1_val11:.3f}  | {F1_O11:.3f}  | {F1_T11:.3f}  |  {F1_P11:.3f}  | {F1_SEPA11:.3f}     | {F1_S11:.3f}  |{F1_C11:.3f}   |{F1_D11:.3f} |')\n",
        "print(f'Model12\\t| {F1_val12:.3f}  | {F1_O12:.3f}  | {F1_T12:.3f}  |  {F1_P12:.3f}  |   {F1_SEPA12:.3f}   | {F1_S12:.3f}  | {F1_C12:.3f}  | {F1_D12:.3f} |')\n",
        "print(f'Model13\\t| {F1_val13:.3f}  | {F1_O13:.3f}  | {F1_T13:.3f}  |  {F1_P13:.3f}  | {F1_SEPA13:.3f}     | {F1_S13:.3f}  |{F1_C13:.3f}   |{F1_D13:.3f} |')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPrNaShn2vnc"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model=model11\n",
        "torch.save(model, '/content/drive/My Drive/Bi-LSTM_Attention_CRF.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxFfy1Zqs0si"
      },
      "source": [
        "### 4.5 Predict on testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es8H4T_atARx"
      },
      "outputs": [],
      "source": [
        "model=model11\n",
        "pred_ls = []\n",
        "\n",
        "for i, sentence in enumerate(test_input_index):\n",
        "    sentence = torch.tensor(sentence,dtype = torch.long).to(device)\n",
        "    pos = torch.tensor(test_pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(test_dep_index[i], dtype=torch.long).to(device)\n",
        "    domain = torch.tensor(test_domain_index[i], dtype=torch.long).to(device)\n",
        "    predicted = model.forward(sentence,pos,dep,domain)\n",
        "    for j in range(len(predicted)):\n",
        "      for tag, index in tag_to_ix.items():\n",
        "        if index == predicted[j]:\n",
        "            pred_ls.append(tag)\n",
        "\n",
        "predict_table={'ID':[],'Predicted':[]}\n",
        "for i, label in enumerate(pred_ls):\n",
        "  predict_table['ID'].append(i)\n",
        "  predict_table['Predicted'].append(label)\n",
        "df = pd.DataFrame(predict_table)\n",
        "df.to_csv('prediction.csv',index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HyZxatd9yd8P",
        "outputId": "b52c578c-249f-4b6a-8f3b-b87fcc37950b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5493e970-c974-4088-ae1b-5f31d21208dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Id</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2321</th>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2322</th>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2323</th>\n",
              "      <td>SEPA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2324</th>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2325</th>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2326 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5493e970-c974-4088-ae1b-5f31d21208dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5493e970-c974-4088-ae1b-5f31d21208dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5493e970-c974-4088-ae1b-5f31d21208dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Id   Predicted\n",
              "0            T\n",
              "1            O\n",
              "2            S\n",
              "3            O\n",
              "4            T\n",
              "...        ...\n",
              "2321         O\n",
              "2322         O\n",
              "2323      SEPA\n",
              "2324         T\n",
              "2325         P\n",
              "\n",
              "[2326 rows x 1 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1T1KYrIOomvx",
        "NRUnCKFXffhq",
        "aqUT6ZPFiE61",
        "zYUTyNoHAIil",
        "5xx0LfA4AQMU"
      ],
      "name": "Copy of 5046 ASM2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
